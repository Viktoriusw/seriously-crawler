# üöÄ Gu√≠a de Inicio R√°pido - SEO Crawler Professional

## ‚ö° Instalaci√≥n Express (3 minutos)

### ü™ü Windows

```bash
# 1. Navegar al proyecto
cd C:\ruta\a\crawlerserio

# 2. Crear entorno virtual
python -m venv venv

# 3. Activar entorno virtual
venv\Scripts\activate
# (Deber√≠as ver "(venv)" al inicio de tu l√≠nea de comandos)

# 4. Actualizar pip
python -m pip install --upgrade pip

# 5. Instalar dependencias
pip install -r requirements.txt

# 6. Lanzar interfaz gr√°fica
python seo_crawler/main.py gui
```

**üí° Nota:** Si tienes error de permisos en PowerShell, ejecuta como Administrador:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### üêß Linux

```bash
# 1. Navegar al proyecto
cd /ruta/a/crawlerserio

# 2. Instalar dependencias del sistema (solo primera vez)
sudo apt-get update && sudo apt-get install python3 python3-pip python3-venv python3-tk

# 3. Crear entorno virtual
python3 -m venv venv

# 4. Activar entorno virtual
source venv/bin/activate
# (Deber√≠as ver "(venv)" al inicio de tu terminal)

# 5. Actualizar pip
pip install --upgrade pip

# 6. Instalar dependencias
pip install -r requirements.txt

# 7. Lanzar interfaz gr√°fica
python seo_crawler/main.py gui
```

**üöÄ Atajo con script de instalaci√≥n:**
```bash
./setup.sh
source venv/bin/activate
python seo_crawler/main.py gui
```

### üçé macOS

```bash
# 1. Navegar al proyecto
cd /ruta/a/crawlerserio

# 2. Crear entorno virtual
python3 -m venv venv

# 3. Activar entorno virtual
source venv/bin/activate
# (Deber√≠as ver "(venv)" al inicio de tu terminal)

# 4. Actualizar pip
pip install --upgrade pip

# 5. Instalar dependencias
pip install -r requirements.txt

# 6. Lanzar interfaz gr√°fica
python seo_crawler/main.py gui
```

---

## üîÑ Uso Diario

### Cada vez que quieras usar el programa:

**Windows:**
```bash
cd C:\ruta\a\crawlerserio
venv\Scripts\activate          # ‚Üê Activar entorno
python seo_crawler/main.py gui  # ‚Üê Lanzar programa
```

**Linux/Mac:**
```bash
cd /ruta/a/crawlerserio
source venv/bin/activate        # ‚Üê Activar entorno
python seo_crawler/main.py gui  # ‚Üê Lanzar programa
```

### Al terminar:
```bash
deactivate  # Desactivar entorno virtual
```

### ‚úÖ Verificar si el entorno est√° activado:
Busca `(venv)` al inicio de tu l√≠nea de comandos:
```
(venv) C:\crawlerserio>  ‚Üê ‚úÖ Activado
C:\crawlerserio>         ‚Üê ‚ùå Desactivado
```

## Primer Crawl desde CLI

```bash
# Crawl b√°sico de un sitio
python seo_crawler/main.py crawl --url https://ejemplo.com --max-pages 50

# Ver sesiones
python seo_crawler/main.py list

# Analizar resultados
python seo_crawler/main.py analyze --session 1

# Generar reporte HTML
python seo_crawler/main.py report --session 1 --format html
```

## Primer Crawl desde GUI

1. **Lanzar GUI:**
   ```bash
   python seo_crawler/main.py gui
   ```

2. **Configurar crawl:**
   - Pesta√±a "üöÄ Nuevo Crawl"
   - Ingresar URL (ej: https://ejemplo.com)
   - Ajustar par√°metros (p√°ginas, profundidad, etc.)
   - Click en "‚ñ∂ Iniciar Crawl"

3. **Ver progreso:**
   - Observar estad√≠sticas en tiempo real
   - Seguir el log de actividad

4. **Analizar resultados:**
   - Pesta√±a "üìã Sesiones" - Ver todas las sesiones
   - Pesta√±a "üìà An√°lisis" - Explorar keywords
   - Pesta√±a "üíæ Exportar" - Generar reportes

## Uso Program√°tico

```python
import asyncio
from seo_crawler.config.settings import Config
from seo_crawler.crawler.core import SEOCrawler

async def crawl_example():
    # Configuraci√≥n
    config = Config({'max_pages': 100, 'max_depth': 3})

    # Crear crawler
    crawler = SEOCrawler(config)

    # Inicializar y crawlear
    await crawler.initialize(['https://ejemplo.com'])
    stats = await crawler.start()

    # Ver resultados
    print(f"P√°ginas: {stats['pages_crawled']}")
    print(f"Keywords: {stats['total_keywords']}")

    # Cleanup
    await crawler.cleanup()

# Ejecutar
asyncio.run(crawl_example())
```

## Ejemplos de Uso

Ver archivo `example_usage.py` para ejemplos completos de:
- Crawling b√°sico
- An√°lisis de sesiones
- Exportaci√≥n de reportes
- Comparaci√≥n de dominios
- An√°lisis personalizado

```bash
python example_usage.py
```

## Casos de Uso Comunes

### 1. An√°lisis Competitivo

```bash
# Crawlear tu sitio
python seo_crawler/main.py crawl --url https://miempresa.com --max-pages 200

# Crawlear competidor
python seo_crawler/main.py crawl --url https://competidor.com --max-pages 200

# Comparar keywords
python seo_crawler/main.py compare --sessions 1 2 --export comparacion.xlsx
```

### 2. Auditor√≠a SEO

```bash
# Crawl completo
python seo_crawler/main.py crawl --url https://misite.com --max-depth 5 --max-pages 500

# Generar reporte
python seo_crawler/main.py report --session 1 --format html
```

### 3. Research de Keywords

```bash
# Crawlear m√∫ltiples sitios del sector
python seo_crawler/main.py crawl --url https://sitio1.com
python seo_crawler/main.py crawl --url https://sitio2.com
python seo_crawler/main.py crawl --url https://sitio3.com

# Encontrar keywords comunes del sector
python seo_crawler/main.py compare --sessions 1 2 3
```

## Estructura de Datos

Los datos se almacenan en `seo_crawler/data/seo_crawler.db` (SQLite).

**Tablas principales:**
- `crawl_sessions` - Sesiones de crawling
- `pages` - P√°ginas crawleadas
- `keywords` - Keywords extra√≠das con TF-IDF
- `links` - Enlaces encontrados
- `images` - Im√°genes con alt text
- `metadata` - Meta tags y structured data

## Reportes Generados

### HTML Report
- Reporte visual completo
- Estad√≠sticas de la sesi√≥n
- Top keywords con m√©tricas
- Dise√±o profesional

### Excel Export
- M√∫ltiples hojas (Session Info, Pages, Keywords)
- Datos estructurados
- F√°cil de analizar en Excel/LibreOffice

### CSV Export
- Keywords con todas las m√©tricas
- Importable a Google Sheets
- Compatible con an√°lisis en Python/R

### JSON Export
- Datos completos en formato JSON
- Integrable con APIs
- Procesamiento program√°tico

## Configuraci√≥n Avanzada

Edita `seo_crawler/config/settings.py` para personalizar:

```python
DEFAULT_CONFIG = {
    'max_pages': 500,              # Cambiar l√≠mite de p√°ginas
    'concurrent_requests': 10,      # Ajustar concurrencia
    'crawl_delay': 1.0,            # Delay entre requests
    'stop_words_language': 'spanish', # Idioma stop words
    'min_keyword_length': 3,       # Longitud m√≠nima keyword
}
```

## Soluci√≥n de Problemas

### Error: ModuleNotFoundError
```bash
pip install -r requirements.txt
```

### GUI no se abre
```bash
# Linux: Instalar tkinter
sudo apt-get install python3-tk

# Verificar
python -m tkinter
```

### Crawl muy lento
```bash
# Aumentar concurrencia y reducir delay
python seo_crawler/main.py crawl --url https://ejemplo.com \
    --concurrent 20 --delay 0.3
```

## Soporte

- üìñ Documentaci√≥n completa: `README.md`
- üí° Ejemplos de c√≥digo: `example_usage.py`
- üêõ Reportar bugs: GitHub Issues

## Pr√≥ximos Pasos

1. ‚úÖ Ejecutar tu primer crawl
2. üìä Analizar los resultados
3. üíæ Exportar reportes
4. üîç Comparar con competidores
5. üöÄ Automatizar con cron/scheduler

---

**¬°Listo para analizar keywords como un profesional!** üéØ
