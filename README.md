# ğŸš€ SEO Crawler Professional

**Herramienta profesional de anÃ¡lisis SEO y extracciÃ³n de keywords** diseÃ±ada para especialistas en SEO, marketers digitales y analistas de contenido que necesitan realizar anÃ¡lisis competitivo, auditorÃ­as SEO y research de keywords a escala.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-production-success.svg)

## âœ¨ CaracterÃ­sticas Principales

### ğŸ” Crawling Avanzado
- âš¡ **Crawling asÃ­ncrono** con aiohttp (10+ requests concurrentes)
- ğŸ¤– **Respeto automÃ¡tico de robots.txt** con cachÃ© por dominio
- ğŸ¯ **Rate limiting configurable** para evitar sobrecarga de servidores
- ğŸ”„ **Manejo inteligente de redirects** y errores HTTP
- ğŸ“Š **DeduplicaciÃ³n de URLs** y detecciÃ³n de contenido duplicado
- ğŸŒ **Soporte multi-dominio** con control de subdominios

### ğŸ“ˆ AnÃ¡lisis de Keywords
- ğŸ§® **CÃ¡lculo de TF-IDF** (Term Frequency-Inverse Document Frequency)
- ğŸ“ **ExtracciÃ³n de n-gramas** (1-gram, 2-gram, 3-gram)
- ğŸ¯ **AnÃ¡lisis de posicionamiento** (title, h1, primeras 100 palabras)
- ğŸ“Š **Densidad de keywords** y detecciÃ³n de keyword stuffing
- ğŸ” **Keyword gaps** (comparaciÃ³n competitiva)
- ğŸ“‰ **Stop words** configurables (espaÃ±ol e inglÃ©s)

### ğŸ’¾ Almacenamiento
- ğŸ—„ï¸ **Base de datos SQLite** con esquema optimizado
- ğŸ“‘ **Almacenamiento estructurado** de pÃ¡ginas, keywords, enlaces e imÃ¡genes
- ğŸ”— **Relaciones normalizadas** para consultas eficientes
- ğŸ“Š **HistÃ³rico de sesiones** para anÃ¡lisis temporal

### ğŸ“Š Reportes y ExportaciÃ³n
- ğŸ“„ **Reportes HTML** visuales y profesionales
- ğŸ“ˆ **ExportaciÃ³n a Excel** con mÃºltiples hojas
- ğŸ’¾ **ExportaciÃ³n CSV** de keywords
- ğŸ“¦ **ExportaciÃ³n JSON** completa
- ğŸ“Š **ComparaciÃ³n entre dominios** y anÃ¡lisis competitivo

### ğŸ–¥ï¸ Interfaces
- ğŸ’» **CLI completa** con mÃºltiples comandos
- ğŸ¨ **GUI profesional con Tkinter** para uso visual
- ğŸ“Š **VisualizaciÃ³n en tiempo real** del progreso
- ğŸ“ˆ **EstadÃ­sticas live** durante el crawl

---

## ğŸ“¦ InstalaciÃ³n Completa con Entorno Virtual

### âš ï¸ Â¿Por quÃ© usar un entorno virtual (venv)?

Un entorno virtual es **altamente recomendado** porque:
- âœ… AÃ­sla las dependencias del proyecto de tu sistema Python global
- âœ… Evita conflictos entre versiones de librerÃ­as
- âœ… Permite tener diferentes versiones de paquetes en diferentes proyectos
- âœ… Facilita la gestiÃ³n de dependencias
- âœ… Es una best practice en desarrollo Python

### ğŸ“‹ Requisitos Previos

Antes de comenzar, asegÃºrate de tener:

1. **Python 3.8 o superior**
2. **pip** (gestor de paquetes, viene con Python)
3. **venv** (mÃ³dulo para entornos virtuales, incluido en Python 3.3+)
4. **tkinter** (para la interfaz grÃ¡fica, generalmente incluido con Python)

#### Verificar Python y pip

```bash
# Verificar versiÃ³n de Python (debe ser 3.8 o superior)
python --version
# o en algunos sistemas:
python3 --version

# Verificar pip
pip --version
# o
pip3 --version
```

Si no tienes Python instalado, descÃ¡rgalo desde [python.org](https://www.python.org/downloads/)

---

## ğŸªŸ InstalaciÃ³n en Windows

### Paso 1: Descargar el proyecto

```bash
# Navegar a la carpeta del proyecto
cd C:\ruta\a\crawlerserio
```

### Paso 2: Crear entorno virtual

```bash
# Crear entorno virtual llamado 'venv'
python -m venv venv
```

**Nota:** Si tienes tanto Python 2 como 3, usa `python3 -m venv venv`

### Paso 3: Activar entorno virtual

```bash
# Activar el entorno virtual
venv\Scripts\activate
```

**Â¿CÃ³mo saber si estÃ¡ activado?**
- VerÃ¡s `(venv)` al inicio de tu lÃ­nea de comandos
- Ejemplo: `(venv) C:\ruta\a\crawlerserio>`

**Si aparece un error de permisos (PowerShell):**
```powershell
# Ejecutar PowerShell como Administrador y ejecutar:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Luego volver a intentar activar:
venv\Scripts\activate
```

**Alternativa con Command Prompt (cmd):**
```bash
venv\Scripts\activate.bat
```

### Paso 4: Actualizar pip (recomendado)

```bash
# Actualizar pip a la Ãºltima versiÃ³n
python -m pip install --upgrade pip
```

### Paso 5: Instalar dependencias

```bash
# Instalar todas las dependencias del proyecto
pip install -r requirements.txt
```

Este proceso puede tardar 1-2 minutos dependiendo de tu conexiÃ³n a internet.

### Paso 6: Verificar instalaciÃ³n

```bash
# Verificar que todo funciona correctamente
python seo_crawler/main.py --help
```

Si ves el menÃº de ayuda del programa, Â¡la instalaciÃ³n fue exitosa! âœ…

### Paso 7: Lanzar el programa

```bash
# OpciÃ³n 1: Interfaz grÃ¡fica
python seo_crawler/main.py gui

# OpciÃ³n 2: LÃ­nea de comandos
python seo_crawler/main.py crawl --url https://ejemplo.com
```

### ğŸ”´ Desactivar entorno virtual (cuando termines)

```bash
deactivate
```

VerÃ¡s que `(venv)` desaparece de tu lÃ­nea de comandos.

---

## ğŸ§ InstalaciÃ³n en Linux

### Paso 1: Descargar el proyecto

```bash
# Navegar a la carpeta del proyecto
cd /ruta/a/crawlerserio
```

### Paso 2: Instalar dependencias del sistema (si es necesario)

```bash
# En Ubuntu/Debian
sudo apt-get update
sudo apt-get install python3 python3-pip python3-venv python3-tk

# En Fedora/RHEL
sudo dnf install python3 python3-pip python3-tkinter

# En Arch Linux
sudo pacman -S python python-pip tk
```

### Paso 3: Crear entorno virtual

```bash
# Crear entorno virtual
python3 -m venv venv
```

### Paso 4: Activar entorno virtual

```bash
# Activar el entorno virtual
source venv/bin/activate
```

**Â¿CÃ³mo saber si estÃ¡ activado?**
- VerÃ¡s `(venv)` al inicio de tu lÃ­nea de comandos
- Ejemplo: `(venv) usuario@pc:~/crawlerserio$`

### Paso 5: Actualizar pip

```bash
# Actualizar pip
pip install --upgrade pip
```

### Paso 6: Instalar dependencias

```bash
# Instalar todas las dependencias
pip install -r requirements.txt
```

### Paso 7: Verificar instalaciÃ³n

```bash
# Verificar que todo funciona
python seo_crawler/main.py --help
```

### Paso 8: Lanzar el programa

```bash
# Interfaz grÃ¡fica
python seo_crawler/main.py gui

# LÃ­nea de comandos
python seo_crawler/main.py crawl --url https://ejemplo.com
```

### ğŸ”´ Desactivar entorno virtual

```bash
deactivate
```

### ğŸš€ Script de instalaciÃ³n automÃ¡tica (Linux)

TambiÃ©n puedes usar el script de instalaciÃ³n incluido:

```bash
# Dar permisos de ejecuciÃ³n
chmod +x setup.sh

# Ejecutar instalaciÃ³n
./setup.sh

# Activar entorno virtual
source venv/bin/activate
```

---

## ğŸ InstalaciÃ³n en macOS

### Paso 1: Instalar Python (si no lo tienes)

```bash
# OpciÃ³n 1: Desde python.org
# Descargar desde https://www.python.org/downloads/macos/

# OpciÃ³n 2: Con Homebrew (recomendado)
brew install python3
```

### Paso 2: Navegar al proyecto

```bash
cd /ruta/a/crawlerserio
```

### Paso 3: Crear entorno virtual

```bash
# Crear entorno virtual
python3 -m venv venv
```

### Paso 4: Activar entorno virtual

```bash
# Activar el entorno virtual
source venv/bin/activate
```

**VerificaciÃ³n:** DeberÃ­as ver `(venv)` al inicio de tu terminal.

### Paso 5: Actualizar pip

```bash
pip install --upgrade pip
```

### Paso 6: Instalar dependencias

```bash
pip install -r requirements.txt
```

### Paso 7: Verificar instalaciÃ³n

```bash
python seo_crawler/main.py --help
```

### Paso 8: Lanzar el programa

```bash
# Interfaz grÃ¡fica
python seo_crawler/main.py gui

# LÃ­nea de comandos
python seo_crawler/main.py crawl --url https://ejemplo.com
```

### ğŸ”´ Desactivar entorno virtual

```bash
deactivate
```

---

## ğŸ”§ SoluciÃ³n de Problemas Comunes

### âŒ Error: "python: command not found"

**SoluciÃ³n:**
```bash
# Intenta con python3 en lugar de python
python3 --version

# Si funciona, usa python3 en todos los comandos
python3 -m venv venv
python3 seo_crawler/main.py gui
```

### âŒ Error: "No module named 'venv'"

**SoluciÃ³n en Linux:**
```bash
sudo apt-get install python3-venv
```

**SoluciÃ³n en Windows:**
- Reinstala Python desde python.org asegurÃ¡ndote de marcar "Add Python to PATH"

### âŒ Error: "No module named 'tkinter'"

**SoluciÃ³n en Linux:**
```bash
sudo apt-get install python3-tk
```

**SoluciÃ³n en macOS:**
```bash
# Tkinter viene con Python, pero si no funciona:
brew reinstall python-tk
```

**En Windows:** Tkinter viene incluido. Si no funciona, reinstala Python marcando la opciÃ³n "tcl/tk and IDLE".

### âŒ Error: "Cannot activate venv on Windows"

**SoluciÃ³n:**
```powershell
# En PowerShell (como Administrador):
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# O usa Command Prompt (cmd) en lugar de PowerShell:
venv\Scripts\activate.bat
```

### âŒ Error: "pip: command not found"

**SoluciÃ³n:**
```bash
# Usa python -m pip en lugar de pip
python -m pip install -r requirements.txt
```

### âŒ El entorno virtual no se activa

**Verificar:**
```bash
# Windows
dir venv\Scripts

# Linux/Mac
ls venv/bin
```

DeberÃ­as ver archivos como `activate`, `python`, etc. Si no existen, vuelve a crear el entorno:

```bash
# Eliminar carpeta venv
rm -rf venv  # Linux/Mac
rmdir /s venv  # Windows

# Volver a crear
python -m venv venv
```

### âŒ "ModuleNotFoundError" al ejecutar el programa

**SoluciÃ³n:**
```bash
# AsegÃºrate de que el entorno virtual estÃ¡ activado
# DeberÃ­as ver (venv) en tu terminal

# Reinstalar dependencias
pip install -r requirements.txt

# Verificar que las dependencias se instalaron
pip list
```

---

## ğŸ“ Uso del Entorno Virtual - Resumen RÃ¡pido

### Cada vez que quieras usar el programa:

**Windows:**
```bash
cd C:\ruta\a\crawlerserio
venv\Scripts\activate
python seo_crawler/main.py gui
```

**Linux/Mac:**
```bash
cd /ruta/a/crawlerserio
source venv/bin/activate
python seo_crawler/main.py gui
```

### Cuando termines:

```bash
deactivate
```

### Verificar si el entorno estÃ¡ activado:

Mira tu lÃ­nea de comandos. Si ves `(venv)` al inicio, estÃ¡ activado:
```
(venv) C:\crawlerserio>        # âœ… Activado
C:\crawlerserio>               # âŒ Desactivado
```

---

## ğŸ¯ VerificaciÃ³n Final de InstalaciÃ³n

Ejecuta estos comandos para verificar que todo estÃ¡ correcto:

```bash
# 1. Verificar Python
python --version

# 2. Verificar entorno virtual activado
# DeberÃ­as ver (venv) en tu terminal

# 3. Verificar dependencias instaladas
pip list | grep aiohttp
pip list | grep beautifulsoup4
pip list | grep pandas

# 4. Verificar que el programa arranca
python seo_crawler/main.py --help

# 5. Verificar GUI (debe abrir una ventana)
python seo_crawler/main.py gui
```

Si todos estos comandos funcionan, Â¡estÃ¡s listo para usar el SEO Crawler! ğŸ‰

---

## ğŸš€ GuÃ­a de Uso

### 1ï¸âƒ£ Interfaz GrÃ¡fica (GUI)

La forma mÃ¡s sencilla de usar el crawler es mediante la interfaz grÃ¡fica:

```bash
python seo_crawler/main.py gui
```

**Funcionalidades de la GUI:**
- âœ… Configurar crawls visualmente
- ğŸ“Š Ver progreso en tiempo real
- ğŸ“‹ Explorar sesiones anteriores
- ğŸ” Analizar keywords con filtros
- ğŸ’¾ Exportar reportes con un clic

![GUI Screenshot](docs/gui_screenshot.png)

### 2ï¸âƒ£ Interfaz de LÃ­nea de Comandos (CLI)

Para usuarios avanzados y automatizaciÃ³n:

#### Crawlear un sitio web

```bash
# Crawl bÃ¡sico
python seo_crawler/main.py crawl --url https://ejemplo.com

# Crawl con opciones personalizadas
python seo_crawler/main.py crawl \
    --url https://ejemplo.com \
    --max-pages 200 \
    --max-depth 3 \
    --concurrent 15 \
    --delay 0.5 \
    --auto-export
```

**Opciones disponibles:**
- `--url`: URL inicial del crawl (requerido)
- `--max-pages`: MÃ¡ximo de pÃ¡ginas a crawlear (default: 500)
- `--max-depth`: Profundidad mÃ¡xima (default: 5)
- `--concurrent`: Requests concurrentes (default: 10)
- `--delay`: Segundos entre requests (default: 1.0)
- `--ignore-robots`: Ignorar robots.txt
- `--follow-external`: Seguir enlaces externos
- `--auto-export`: Generar reporte HTML automÃ¡ticamente

#### Listar sesiones

```bash
python seo_crawler/main.py list
```

Output:
```
================================================================================
SESIONES DE CRAWLING
================================================================================
  ID | Dominios                       | PÃ¡ginas | Keywords | Fecha
--------------------------------------------------------------------------------
   1 | ejemplo.com                    |     150 |     2340 | 2024-01-11 10:30:00
   2 | competencia.com                |     200 |     3150 | 2024-01-11 14:15:00
================================================================================
```

#### Analizar una sesiÃ³n

```bash
# AnÃ¡lisis en consola
python seo_crawler/main.py analyze --session 1

# AnÃ¡lisis con exportaciÃ³n
python seo_crawler/main.py analyze --session 1 --export resultados.xlsx
```

#### Generar reportes

```bash
# Reporte HTML
python seo_crawler/main.py report --session 1 --format html

# Reporte Excel
python seo_crawler/main.py report --session 1 --format excel --output reporte.xlsx

# CSV de keywords
python seo_crawler/main.py report --session 1 --format csv

# JSON completo
python seo_crawler/main.py report --session 1 --format json
```

#### Comparar sesiones

```bash
# Comparar 2 sesiones (anÃ¡lisis detallado)
python seo_crawler/main.py compare --sessions 1 2

# Comparar mÃºltiples sesiones
python seo_crawler/main.py compare --sessions 1 2 3

# Comparar y exportar
python seo_crawler/main.py compare --sessions 1 2 --export comparacion.xlsx
```

---

## ğŸ“ Estructura del Proyecto

```
crawlerserio/
â”œâ”€â”€ seo_crawler/
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py              # Motor principal del crawler
â”‚   â”‚   â”œâ”€â”€ robots.py            # Gestor de robots.txt
â”‚   â”‚   â”œâ”€â”€ url_manager.py       # Cola de URLs y deduplicaciÃ³n
â”‚   â”‚   â””â”€â”€ rate_limiter.py      # Control de velocidad por dominio
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ html_parser.py       # ExtracciÃ³n de elementos HTML
â”‚   â”‚   â”œâ”€â”€ keyword_analyzer.py  # AnÃ¡lisis de keywords y TF-IDF
â”‚   â”‚   â””â”€â”€ metadata_extractor.py # Meta tags, structured data
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py          # GestiÃ³n de SQLite
â”‚   â”‚   â””â”€â”€ schemas.py           # Esquemas de base de datos
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ keyword_metrics.py   # MÃ©tricas de keywords
â”‚   â”‚   â””â”€â”€ reporter.py          # Generador de reportes
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py          # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ helpers.py           # Funciones auxiliares
â”‚   â”œâ”€â”€ gui/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main_window.py       # Interfaz grÃ¡fica Tkinter
â”‚   â”œâ”€â”€ data/                    # Base de datos y logs (generado)
â”‚   â””â”€â”€ main.py                  # Punto de entrada CLI
â”œâ”€â”€ requirements.txt             # Dependencias
â””â”€â”€ README.md                    # Este archivo
```

---

## ğŸ¯ Casos de Uso

### 1. AnÃ¡lisis Competitivo

```bash
# Crawlear tu sitio
python seo_crawler/main.py crawl --url https://miempresa.com --max-pages 300

# Crawlear competidor
python seo_crawler/main.py crawl --url https://competencia.com --max-pages 300

# Comparar y encontrar keyword gaps
python seo_crawler/main.py compare --sessions 1 2
```

**Resultado:** IdentificarÃ¡s keywords que usa tu competencia y tÃº no, permitiÃ©ndote optimizar tu estrategia de contenido.

### 2. AuditorÃ­a SEO On-Page

```bash
# Crawl profundo del sitio
python seo_crawler/main.py crawl --url https://misite.com --max-depth 5

# Analizar y exportar
python seo_crawler/main.py analyze --session 1 --export auditoria.xlsx
```

**Resultado:** Excel con todas las pÃ¡ginas, sus keywords, densidades, y detecciÃ³n de keyword stuffing.

### 3. Research de Keywords

```bash
# Crawlear mÃºltiples sitios del sector
python seo_crawler/main.py crawl --url https://sitio1.com
python seo_crawler/main.py crawl --url https://sitio2.com
python seo_crawler/main.py crawl --url https://sitio3.com

# Encontrar keywords comunes
python seo_crawler/main.py compare --sessions 1 2 3
```

**Resultado:** Lista de keywords mÃ¡s relevantes del sector con mÃ©tricas TF-IDF.

### 4. Monitoreo Temporal

```bash
# Crawlear el mismo sitio periÃ³dicamente
python seo_crawler/main.py crawl --url https://misite.com

# Comparar con crawl anterior
python seo_crawler/main.py compare --sessions 5 10
```

**Resultado:** Detectar cambios en la estrategia de keywords del sitio a lo largo del tiempo.

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Personalizar configuraciÃ³n

Puedes modificar `seo_crawler/config/settings.py` para ajustar:

```python
DEFAULT_CONFIG = {
    # Crawling
    'max_pages': 500,
    'max_depth': 5,
    'concurrent_requests': 10,
    'crawl_delay': 1.0,

    # Keywords
    'min_keyword_length': 3,
    'max_keyword_length': 50,
    'stop_words_language': 'spanish',  # 'spanish' o 'english'
    'max_ngram_size': 3,

    # Base de datos
    'database_path': 'data/seo_crawler.db',

    # ExportaciÃ³n
    'export_formats': ['csv', 'excel', 'json', 'html'],
}
```

### Patrones de exclusiÃ³n

AÃ±ade patrones regex para excluir URLs:

```python
'exclude_patterns': [
    r'.*\.(jpg|jpeg|png|gif|pdf|doc|docx|zip|rar)$',
    r'.*/feed/?$',
    r'.*/wp-admin/.*',
    r'.*/wp-json/.*',
    r'.*/tag/.*',
    r'.*/category/.*'
]
```

---

## ğŸ“Š Esquema de Base de Datos

### Tablas Principales

**crawl_sessions**
- `session_id`: ID Ãºnico de la sesiÃ³n
- `seed_url`: URL inicial
- `domains`: Dominios crawleados
- `start_time`, `end_time`: Timestamps
- `pages_crawled`, `total_keywords`: EstadÃ­sticas

**pages**
- `page_id`: ID Ãºnico
- `url`, `domain`: Identificadores
- `title`, `h1`, `meta_description`: Elementos SEO
- `word_count`, `content_hash`: MÃ©tricas
- `status_code`, `response_time`: Info tÃ©cnica

**keywords**
- `keyword_id`: ID Ãºnico
- `keyword`: Palabra clave
- `frequency`: Apariciones
- `density`: % de densidad
- `tf_idf_score`: Score TF-IDF
- `position_in_title`, `position_in_h1`: Posicionamiento
- `is_ngram`, `ngram_size`: Tipo de keyword

**links**
- `link_id`: ID Ãºnico
- `source_page_id`: PÃ¡gina origen
- `target_url`: URL destino
- `anchor_text`: Texto ancla
- `is_internal`, `nofollow`: Atributos

---

## ğŸ”§ Desarrollo y ExtensiÃ³n

### AÃ±adir nuevos extractores

```python
# En extractors/custom_extractor.py
class CustomExtractor:
    def extract(self, html: str) -> dict:
        # Tu lÃ³gica personalizada
        return data
```

### Crear nuevas mÃ©tricas

```python
# En analytics/custom_metrics.py
class CustomMetrics:
    async def calculate_metric(self, session_id: int):
        # Tus cÃ¡lculos personalizados
        return metric_data
```

### Tests (opcional)

```bash
# Instalar dependencias de testing
pip install pytest pytest-asyncio

# Ejecutar tests
pytest tests/
```

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Haz fork del proyecto
2. Crea una branch para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## ğŸ“ Roadmap

### PrÃ³ximas funcionalidades
- [ ] IntegraciÃ³n con APIs de Google Search Console
- [ ] ExportaciÃ³n a bases de datos externas (MySQL, PostgreSQL)
- [ ] GeneraciÃ³n de grÃ¡ficos y visualizaciones avanzadas
- [ ] Soporte para JavaScript rendering (Playwright/Selenium)
- [ ] AnÃ¡lisis de imÃ¡genes (alt text, tamaÃ±o, optimizaciÃ³n)
- [ ] DetecciÃ³n de errores tÃ©cnicos SEO (canonical, hreflang, etc.)
- [ ] CLI interactiva con menÃºs
- [ ] API REST para integraciÃ³n con otras herramientas
- [ ] Dashboard web con Flask/Django

---

## âš ï¸ Consideraciones Legales y Ã‰ticas

### Uso Responsable

Esta herramienta estÃ¡ diseÃ±ada para:
- âœ… AnÃ¡lisis de tus propios sitios web
- âœ… AnÃ¡lisis competitivo con fines educativos/investigaciÃ³n
- âœ… AuditorÃ­as SEO autorizadas
- âœ… Research de mercado legÃ­timo

**NO debe usarse para:**
- âŒ Sobrecarga de servidores (DoS)
- âŒ Scraping masivo no autorizado
- âŒ ViolaciÃ³n de tÃ©rminos de servicio
- âŒ Acceso a contenido protegido

### Respeto de robots.txt

Por defecto, el crawler **respeta robots.txt**. Solo desactiva esta opciÃ³n si:
- Tienes permiso explÃ­cito del propietario del sitio
- Es tu propio sitio
- EstÃ¡s en un entorno de testing

### Rate Limiting

El crawler incluye rate limiting por defecto (1 req/seg) para evitar sobrecargar servidores. Ajusta este valor responsablemente.

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "ModuleNotFoundError"

```bash
# AsegÃºrate de haber instalado las dependencias
pip install -r requirements.txt
```

### Error: "Database locked"

```bash
# Cierra todas las instancias del programa y elimina locks
rm seo_crawler/data/*.db-wal
rm seo_crawler/data/*.db-shm
```

### Crawl muy lento

```bash
# Aumenta requests concurrentes y reduce delay
python seo_crawler/main.py crawl --url https://ejemplo.com \
    --concurrent 20 --delay 0.3
```

### GUI no se abre

```bash
# Verifica que tkinter estÃ© instalado
python -m tkinter

# En Linux, instalar tkinter si falta
sudo apt-get install python3-tk
```

---

## ğŸ“ Soporte

Para reportar bugs, solicitar features o hacer preguntas:

- ğŸ“§ Email: [tu-email@ejemplo.com]
- ğŸ› Issues: [GitHub Issues](https://github.com/usuario/seo-crawler/issues)
- ğŸ“– DocumentaciÃ³n: [Wiki del proyecto](https://github.com/usuario/seo-crawler/wiki)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ™ Agradecimientos

Este proyecto utiliza las siguientes bibliotecas de cÃ³digo abierto:

- [aiohttp](https://github.com/aio-libs/aiohttp) - HTTP asÃ­ncrono
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - Parseo HTML
- [pandas](https://pandas.pydata.org/) - AnÃ¡lisis de datos
- [tkinter](https://docs.python.org/3/library/tkinter.html) - GUI

---

## ğŸŒŸ CaracterÃ­sticas TÃ©cnicas Destacadas

### Arquitectura
- âœ¨ **CÃ³digo asÃ­ncrono** con asyncio para mÃ¡ximo rendimiento
- ğŸ¯ **Type hints** en todas las funciones (Python 3.8+)
- ğŸ“ **Docstrings completos** en espaÃ±ol
- ğŸ§© **DiseÃ±o modular** fÃ¡cilmente extensible
- ğŸ”’ **Manejo robusto de excepciones**
- ğŸ“Š **Logging estructurado** con mÃºltiples niveles

### Performance
- âš¡ 10+ requests concurrentes por defecto
- ğŸš€ 1-2 pÃ¡ginas/segundo en promedio
- ğŸ’¾ Base de datos optimizada con Ã­ndices
- ğŸ”„ CachÃ© de robots.txt por dominio
- ğŸ“¦ GestiÃ³n eficiente de memoria

---

## ğŸ“š Recursos Adicionales

### Tutoriales
- [CÃ³mo hacer un anÃ¡lisis competitivo de keywords](docs/tutorial_analisis_competitivo.md)
- [Detectar keyword stuffing en tu sitio](docs/tutorial_keyword_stuffing.md)
- [Automatizar crawls con cron/Task Scheduler](docs/tutorial_automatizacion.md)

### Ejemplos de scripts
- [Script para crawl masivo de competidores](examples/crawl_competidores.py)
- [Generar reportes automÃ¡ticos por email](examples/reportes_email.py)
- [IntegraciÃ³n con Google Sheets](examples/export_google_sheets.py)

---

**Desarrollado con â¤ï¸ para profesionales del SEO**

_Ãšltima actualizaciÃ³n: Enero 2024_
